











# -*- coding=utf-8 -*-
# __author = 'bonnieting'__

# basic:
import pandas as pd
pd.set_option('display.float_format',lambda x:'%.2f' %x)
# pd.set_option('display.float_format', '{:.0f}'.format)
import numpy as np
np.set_printoptions(precision=4, suppress=True)
import jieba
import locale
locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
from itertools import product
import warnings
warnings.filterwarnings('ignore')
# time:
from datetime import datetime as dt
import time
from string import ascii_uppercase

# visual:
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

import plotly.express as px
from plotly import graph_objects as go
import plotly.io as pio
pio.renderers.default = 'notebook_connected'
import plotly as ply
ply.offline.init_notebook_mode(connected=True)
from plotly.subplots import make_subplots
import plotly.figure_factory as ff

import seaborn as sns
sns.set(style="whitegrid")

from pyecharts.globals import CurrentConfig, OnlineHostType, NotebookType
CurrentConfig.ONLINE_HOST = OnlineHostType.NOTEBOOK_HOST
CurrentConfig.NOTEBOOK_TYPE = NotebookType.JUPYTER_NOTEBOOK#LAB
from pyecharts import options as opts
from pyecharts.charts import Bar
from pyecharts.charts import HeatMap
from pyecharts.faker import Faker

# model:
from scipy.stats import norm
from scipy.stats.mstats import winsorize
import lifetimes
from lifetimes import plotting
from sklearn.decomposition import PCA

# dataframe show_way:
from IPython.display import Markdown, display, HTML
# from IPython.display import display
# import os
import sys
# sys.path.insert(0, '../common')
# from common_htmlTable import HtmlTableMulti
# from IPython.display import display, display_html


data=pd.read_csv(r"D:\repo_github\Portfolio\Portfolio1_sales_EDA\data\2019-Dec.csv")








data.head()


print(data.shape)
print(data.info())


data.isnull().any()


data.nunique()


data.isnull().sum()











event_frequency = data['user_id'].value_counts()
print('event_frequency: \n' + str(event_frequency))

total_uv = len(event_frequency)
print('\ntotal_uv: '+str(total_uv))

total_pv = len(data['user_id'])
print('\ntotal_pv: '+str(total_pv))





total_order = len(data[data.event_type=='purchase']['user_session'].value_counts())
print('total_order: '+str(total_order))
#delete invalid column 删除无效列
# data.drop('category_code',axis=1,inplace=True)





total_gmv = data[data.event_type=='purchase']['price'].sum()
print('total_gmv: '+str(locale.currency(total_gmv,grouping = True))) # f'{total_gmv:.2f}'


paid_user = data[data.event_type=='purchase'].nunique()['user_id']
Average_Purchase_Amount = total_gmv/paid_user
print('Average Purchase Amount (mean_amt_per_paid_user): ' + str(locale.currency(Average_Purchase_Amount,grouping = True)))





data['price'].describe()








user_event_count = data['event_type'].value_counts()
print(user_event_count)








data[['user_id','product_id','category_id','user_session']] = data[['user_id','product_id','category_id','user_session']].astype(str)
data.describe()


data[data['price']<0]['product_id'].value_counts()





# data['price'] = data['price'].where(data['price']<0, inplace=True)
data['price'] = data['price'].apply(lambda x: max(0, x))
data[data['price']<0]


data[(data['price']==0) & (data['event_type']=='purchase')]['category_id'].value_counts() # ['event_type'].value_counts()





# data[data['brand'].isnull()]
# there're 1510289 rows with no brand info


data[['category_code','brand','user_session']] = data[['category_code','brand','user_session']].fillna('null')
data.isnull().any()
# data[data['brand']=='null']











view_user = data[data.event_type=='view'].nunique()['user_id']
print('view_user: '+str(view_user))
total_view_rate = (view_user/total_uv)*100
print('total_view_rate: '+str('%.2f'%total_view_rate)+'%')

cart_user = data[data.event_type=='cart'].nunique()['user_id']
print('\ncart_user: '+str(cart_user))
total_cart_rate = (cart_user/total_uv)*100
print('total_cart_rate: '+str('%.2f'%total_cart_rate)+'%')
cart_pct_inview = (cart_user/view_user)*100
print('cart_pct_inview: '+str('%.2f'%cart_pct_inview)+'%')

# pre-defined: paid_user = data[data.event_type=='purchase'].nunique()['user_id']
print('\npaid_user: '+str(paid_user)) # '%.0f'%
total_paid_rate = (paid_user/total_uv)*100
print('total_paid_rate: '+str('%.2f'%total_paid_rate)+'%')
paid_pct_incart = (paid_user/cart_user)*100
print('paid_pct_incart: '+str('%.2f'%paid_pct_incart)+'%')
paid_pct_inview = (paid_user/view_user)*100
print('paid_pct_inview: '+str('%.2f'%paid_pct_inview)+'%')








fig = go.Figure(go.Funnel(
    y = ["view", "cart", "purchase"],
    x = [view_user, cart_user, paid_user],
    textposition = "inside",
    textinfo = "value + percent previous",
    opacity = 0.65, marker = {"color": ["lightsalmon", "tan", "teal"],
    "line": {"width": [2, 2, 2], "color": ["wheat", "wheat", "wheat",]}},
    connector = {"line": {"color": "royalblue", "dash": "dot", "width": 3}})
    )
fig.update_layout(width=1000, height=500)
fig.show()








user_buy_times = data[data.event_type=='purchase'].groupby('user_id').count()['user_session'] 
# output Series，should we calculate the frequency in the all "purchase" users or in the all? I think the former making sense.
user_buy_times


user_buy_times.describe()


print('max_buying_times: ' + str(user_buy_times.max()))
print('min_buying_times: ' + str(user_buy_times.min()))
Average_Purchase_Frequency = user_buy_times.mean()
print('Average Purchase Frequency (mean_buying_times_per_paid_user): ' + str('%.2f'%Average_Purchase_Frequency))
# buy_times_over20 = user_buy_times.values > 20
# buy_times_over20 = np.count_nonzero(buy_times_over20)
# buy_times_over20 = np.sum(buy_times_over20)
# buy_times_over20


buy_times_within_20 = len(user_buy_times[(user_buy_times >= 0) & (user_buy_times <= 20)])
print('Buying_times between [0-20]: ' + str(buy_times_within_20))
buy_times_within_20_ratio = (buy_times_within_20 / paid_user)*100
print("Buying_times between [0-20] ratio: ", str('%.2f'%buy_times_within_20_ratio)+'%')





def plot_histogram(ax, data, title, bins):
    n, bins, patches = ax.hist(x=data, bins=bins, density=False)
    ax.set_xlabel('buy_times')
    ax.set_ylabel('user_count')

    bin_centers = 0.5 * (bins[:-1] + bins[1:])
    freq = n / n.sum()

    ax2 = ax.twinx()
    ax2.plot(bin_centers, freq, 'r-', linewidth=1.0)
    ax2.set_ylabel('user_pct')
    ax2.set_ylim([0, freq.max() * 1.1])
    ax.set_title(title)

    return ax, ax2, n, freq

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 3))
plt.subplots_adjust(wspace=0.5)

plot_histogram(axes[0], user_buy_times, 'ALL', np.arange(0, 400, 10))
plot_histogram(axes[1], user_buy_times[user_buy_times.values<51], 'MAIN', np.arange(0, 51, 2))
plot_histogram(axes[2], user_buy_times[user_buy_times.values<11], 'CORE', np.arange(0, 11, 1))

plt.show()


# print('times data was split into ' + str(len(n)) + ' groups by steplength: 10 .')
# freq = freq.tolist()
# freq = [f'{f*100:.2f}'+'%' for f in freq]
# df1 = pd.DataFrame({'Buying_times From': bins[:-1], 'Buying_times To': bins[1:], 'Frequency': n, 'Normalized Frequency': freq})
# df1.head(13)








twice_buy_user=user_buy_times[user_buy_times>=2].count()
twice_buy_user
rebuy_rate=(twice_buy_user/paid_user)*100
print('rebuy_rate: '+str('%.2f'%rebuy_rate)+'%')
# print("复购率:{:.2f}%".format(rebuy_rate*100))








# we have variable: event_frequency = data['user_id'].value_counts() = data.groupby('user_id').count()['user_session']
once_buy_user = user_buy_times[user_buy_times==1].count()
once_buy_rate = (once_buy_user/total_uv)*100
print('once_buy_churn_rate: '+str('%.2f'%once_buy_rate)+'%')


event_type_df = data.groupby(['user_id', 'event_type']).count().reset_index()
df3 = event_type_df[event_type_df['event_type']=='view']['user_id']
user_event_type_nums = event_type_df.groupby('user_id').count()['user_session']
user_event_type_nums
only_view_user = len(set(user_event_type_nums[user_event_type_nums==1].index).intersection(set(df3.values)))
only_view_rate = (only_view_user/total_uv)*100
print('only_view_churn_rate: '+str('%.2f'%only_view_rate)+'%')


df5 = event_type_df[event_type_df['event_type']=='cart']['user_id']
cart_no_buy = len(set(df5.values).difference(set(user_buy_times.index)))
cart_no_buy_rate = (cart_no_buy/len(df5))*100
print(str(cart_no_buy) + ' persons in 83458 cart customers didn\'t go to purchase. The only_cart churn rate for the very cart link is: ' + str('%.2f'%cart_no_buy_rate) + '%.')

cart_no_buy_rate_intotal = (cart_no_buy/total_uv)*100
print('only_cart_churn_rate: ' +str('%.2f'%cart_no_buy_rate_intotal) + '%.')


cart_from_view = len(set(set(df5.values).intersection(set(df3.values))))
cart_from_view_rate = (cart_from_view/len(df5))*100
print(str(cart_from_view) + ' persons in 83458 cart customers came from view. The rate is: ' + str('%.2f'%cart_from_view_rate) + '%.')


print('The total churn rate is: ' + str('%.2f'%sum([once_buy_rate, cart_no_buy_rate_intotal, only_view_rate])) + '%.')











event_time_df = data[data.event_type=='purchase'].groupby('user_id')['event_time'].agg(['min', 'max', 'count']).rename(columns={'min':'min_time', 'max':'max_time', 'count':'buy_time_times'}) # last buy time - first buy time = lifespan (not first/last event time)
# event_time_df['daysdiff'] = dt.date(event_time_df['max_time']) - dt.date(event_time_df['min_time']) # TypeError: 'Series' object cannot be interpreted as an integer
event_time_df['daysdiff'] = (pd.to_datetime(event_time_df['max_time']) - pd.to_datetime(event_time_df['min_time'])).dt.days + 1
event_time_df = event_time_df.sort_values(by='daysdiff', ascending=False)
event_time_df


# effective_event_time_df = event_time_df[event_time_df['daysdiff']>0]
mean_daysdiff = event_time_df['daysdiff'].agg('mean')
print('Average Customer Lifespan: ' + str('%.2f'%mean_daysdiff) + ' days.')
event_time_df['daysdiff'].describe()
# effective_event_time_df['daysdiff'].describe() # same as above


Average_Customer_Lifespan = mean_daysdiff
LTV = Average_Customer_Lifespan * Average_Purchase_Frequency * Average_Purchase_Amount

print('Customer Lifetime Value: $' + str('%.2f'%LTV))


monthly_cvalue = Average_Purchase_Frequency * Average_Purchase_Amount
print('Monthly Customer Value: Per paid customer brought $' + str('%.2f'%monthly_cvalue) + ' in average per month.') 


daysdiff_1 = event_time_df[event_time_df.daysdiff == 1]['daysdiff'].count()
print(daysdiff_1, 'much more than' ,once_buy_user)








# buy_df = data[data.event_type=='purchase'].groupby('user_id')
event_time_df['max_time2'] = pd.to_datetime(event_time_df['max_time'].str[:-4],utc=True, format='%Y-%m-%d %H:%M:%S').dt.tz_localize(None) # YYYY-MM-DDTHH-MM-SS,, errors='coerce'
analysis_date = event_time_df['max_time2'].max().replace(hour=23, minute=59, second=59) # datetime.today().date() or (event_time_df['max_time2'].max() + pd.DateOffset(days=1)).date()
event_time_df['lastbuy_interval'] =  (analysis_date - event_time_df['max_time2']).dt.days
recency_data = event_time_df['lastbuy_interval'].max() - event_time_df['lastbuy_interval'] 
# !!!daysdiff is "lifespan" but imcompatible with authoritative RFM definition of Recency(how recently a client bought a product)
# use recency_data instead of daysdiff so that the larger recency, the higher R-score, that's more resonable now.!!!

monetary_data = data[data.event_type=='purchase'].groupby('user_id')['price'].sum()

rfm_df = pd.DataFrame({'Recency': recency_data, 'Frequency': user_buy_times, 'Monetary': monetary_data})
rfm_df


f, ax = plt.subplots(figsize=(3, 6))
sns.boxplot(rfm_df, width=.6,  palette="vlag", fliersize=2, orient='v') #   whis=[0, 100],[['Recency','Frequency','Monetary']]
sns.stripplot(rfm_df, size=0.5, color=".1")
ax.xaxis.grid(True)
ax.set(ylabel="")
sns.despine(trim=True, left=True)





# exclude extremum values by percentile
recency_pct = rfm_df['Recency'].quantile([0.01, 0.99])
frequency_pct = rfm_df['Frequency'].quantile([0.01, 0.99])
monetary_pct = rfm_df['Monetary'].quantile([0.01, 0.99])

rfm_df = rfm_df[(rfm_df['Recency'] >= recency_pct.iloc[0]) & (rfm_df['Recency'] <= recency_pct.iloc[1])]
rfm_df = rfm_df[(rfm_df['Frequency'] >= frequency_pct.iloc[0]) & (rfm_df['Frequency'] <= frequency_pct.iloc[1])]
rfm_df = rfm_df[(rfm_df['Monetary'] >= monetary_pct.iloc[0]) & (rfm_df['Monetary'] <= monetary_pct.iloc[1])]


rfm_df['R'], r_bins= pd.qcut(rfm_df['Recency'], q=10, labels=False, retbins=True) # labels=True, retbins=True
rfm_df['F'], f_bins = pd.qcut(rfm_df['Frequency'], q=10, labels=False, retbins=True, duplicates='drop')
rfm_df['M'], m_bins = pd.qcut(rfm_df['Monetary'], q=10, labels=False, retbins=True)

rfm_df['RFM_Score'] = 0.2*rfm_df['R'] + 0.3*rfm_df['F'] + 0.5*rfm_df['M'] # [0.1,0.3,0.6]the weight was decided according to what we cared most.
rfm_df['LTV'] = rfm_df['RFM_Score'] * Average_Purchase_Amount

print("Recency bins:", r_bins)
print("Frequency bins:", f_bins)
print("Monetary bins:", m_bins)
# rfm_df = rfm_df.sort_values(by='LTV', ascending=False)
rfm_df # .reset_index()


f, ax = plt.subplots(figsize=(3, 6))
sns.boxplot(rfm_df[['Recency','Frequency','Monetary']], width=.6, whis=1.5, palette="vlag", fliersize=2) #   whis=[0, 100],
sns.stripplot(rfm_df[['Recency','Frequency','Monetary']], size=0.5, color=".1")
ax.xaxis.grid(True)
ax.set(ylabel="")
sns.despine(trim=True, left=True)


rfm_df[['Recency', 'Frequency', 'Monetary','LTV']].describe()








sns.set(style="whitegrid")
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))

sns.histplot(rfm_df['Recency'], bins=30, kde=True, ax=axes[0])
axes[0].set_title('Recency Distribution')

sns.histplot(rfm_df['Frequency'], bins=30, kde=True, ax=axes[1])
axes[1].set_title('Frequency Distribution')

sns.histplot(rfm_df['Monetary'], bins=30, kde=True, ax=axes[2])
axes[2].set_title('Monetary Distribution')

plt.tight_layout()
plt.show()








rfm_df2 = rfm_df.reset_index()
fig = px.scatter_3d(rfm_df2, x='Recency', y='Frequency', z='Monetary', color='RFM_Score',
                    size='RFM_Score', opacity=0.5, size_max=10, hover_name='user_id', #symbol='user_id',
                    title='RFM User Segmentation Cube')

fig.update_layout(scene=dict(xaxis_title='Recency', yaxis_title='Frequency', zaxis_title='Monetary'))
fig.update_layout(width=1000, height=800)

# add straight line to present the 8 subcubes spliting by 50% quantiles above: R-15 F-5 M-28.17
lines_x = [[0,30],[0,30],[0,30],[0,30],[0,30],[0,30],[0,30],[0,30]]
lines_y = [[0,0],[50,50],[0,0],[5,5],[50,50],[0,0],[5,5],[50,50]]
lines_z = [[28.17,28.17],[28.17,28.17],[0,0],[0,0],[0,0],[250,250],[250,250],[250,250]]
for i in range(len(lines_x)):
    fig.add_trace(go.Scatter3d(x=lines_x[i], y=lines_y[i], z=lines_z[i], mode='lines', line=dict(color='black', width=3), name='Line {}'.format(i + 1), showlegend=False))


lines_x = [[0,0],[30,30],[0,0],[15,15],[30,30],[0,0],[15,15],[30,30]]
lines_y = [[0,50],[0,50],[0,50],[0,50],[0,50],[0,50],[0,50],[0,50]]
lines_z = [[28.17,28.17],[28.17,28.17],[0,0],[0,0],[0,0],[250,250],[250,250],[250,250]]
for i in range(len(lines_x)):
    fig.add_trace(go.Scatter3d(x=lines_x[i], y=lines_y[i], z=lines_z[i], mode='lines', line=dict(color='black', width=3), name='Line {}'.format(i + 1), showlegend=False)) 


lines_x = [[15,15],[15,15],[0,0],[0,0],[0,0],[30,30],[30,30],[30,30]]
lines_y = [[0,0],[50,50],[0,0],[5,5],[50,50],[0,0],[5,5],[50,50]]
lines_z = [[0,250],[0,250],[0,250],[0,250],[0,250],[0,250],[0,250],[0,250]]
for i in range(len(lines_x)):
    fig.add_trace(go.Scatter3d(x=lines_x[i], y=lines_y[i], z=lines_z[i], mode='lines', line=dict(color='black', width=3), name='Line {}'.format(i + 1), showlegend=False))


fig.add_trace(go.Scatter3d(x=[15,15], y=[0,50], z=[28.17,28.17], mode='lines', line=dict(color='black', width=3), showlegend=False)) # legendgroup='divide_R', legendrank=1500 , legendgrouptitle={'text':'Segment'}
fig.add_trace(go.Scatter3d(x=[0, 30], y=[5,5], z=[28.17,28.17], mode='lines', line=dict(color='black', width=3), showlegend=False))
fig.add_trace(go.Scatter3d(x=[15, 15], y=[5,5], z=[0, 250], mode='lines', line=dict(color='black', width=3), showlegend=False))

# add center point
fig.add_trace(go.Scatter3d(x=[15], y=[25], z=[125],
                           mode='markers', marker=dict(color='red', size=5),
                           text=['(15,25,125)'], textposition='middle center'))

fig.show()


rfm_df2['group'] = ''
for i in range(0,len(rfm_df2)):
    if rfm_df2['R'][i]<5:
        if rfm_df2['F'][i]<5:
            if rfm_df2['M'][i]<5:
                rfm_df2['group'][i]='000'
            else:
                rfm_df2['group'][i]='001'
        else:
            if rfm_df2['M'][i]<5:
                rfm_df2['group'][i]='010'
            else:
                rfm_df2['group'][i]='011'
    else:
        if rfm_df2['F'][i]<5:
            if rfm_df2['M'][i]<5:
                rfm_df2['group'][i]='100'
            else:
                rfm_df2['group'][i]='101'
        else:
            if rfm_df2['M'][i]<5:
                rfm_df2['group'][i]='110'
            else:
                rfm_df2['group'][i]='111'
rfm_df2['group'] = rfm_df2['group'].astype(str)


rfm_df2


a = [1,0]
b = [1,0]
c=[1,0]

comb = list(product(a, b,c))
gpn_df = pd.DataFrame(np.array(comb), columns=['Rs', 'Fs', 'Ms'])
gpn_df['group'] = gpn_df.apply(lambda row: ''.join(map(str, row)), axis=1)
gpn_df['tags'] = ['champions','promising','potentital_loyalist','newcome','former_loyalist','at_risk','hibernating','almost_lost']
gpn_df['group'] = gpn_df['group'].astype(str)
gpn_df


gpn_df.info()


rfm_df3 = pd.merge(rfm_df2, gpn_df, on='group', how='left')
# rfm_df3 = rfm_df3.drop(['Rs','Fs','Ms'], axis=1, inplace=True)
rfm_df3


precised_LTV = rfm_df3.groupby('tags')['LTV'].mean().sort_values(ascending=False)
precised_LTV





s = rfm_df3['tags'].value_counts()
# fig = px.bar(s, y=s.values, x=s.index, text_auto='.2s',title="User Tags Distribution") #, subtitle='see how many users in each tag')
# fig.update_traces(textfont_size=12, textangle=0, textposition="outside", cliponaxis=False)
# fig.show()
freq = s / s.sum()
formatted = ['{:.2%}'.format(i) for i in freq.values]
# 创建频率分布直方图
fig1 = go.Figure([go.Bar(x=s.index, y=s.values, text=s.values, hoverinfo='none', textposition='auto', yaxis='y1', showlegend=False),
                  go.Scatter(x=s.index, y=freq, hoverinfo='x+text', text=formatted, mode='lines+markers', yaxis='y2', showlegend=False)])
fig1.update_layout(
    title='Frequency Distribution of User Tags',
    xaxis_title='Tags',
    yaxis_title='User Count',
    yaxis2=dict(title='Segment Percentage', overlaying='y', side='right'), overwrite=True)

fig3 = go.Figure(go.Bar(x=precised_LTV.index, y=precised_LTV.values, text=precised_LTV.values.round(2), hoverinfo='all', textposition='auto', textangle=-45, showlegend=False))

fig2 = go.Figure(go.Pie(labels=s.index, values=s.values, showlegend=True))
# test=go.histogram(kde=)
# create a layout with [1,2] grid
fig = make_subplots(rows=1, cols=3, column_widths=[0.4, 0.4, 0.2], specs=[[{"type": "xy","secondary_y":True}, {"type": "xy"}, {"type": "domain"}]]) 
# ↑ has to set "secondary_y":True here to let the below one effective

# add subplots into grid
fig.add_trace(fig1.data[0], row=1, col=1)
fig.add_trace(fig1.data[1], row=1, col=1, secondary_y=True)
fig.add_trace(fig3.data[0], row=1, col=2)
fig.add_trace(fig2.data[0], row=1, col=3)
fig.update_layout(title_text="Frequency Distribution Chart + Pie Chart + LTV histogram of User_Tag Groups")

fig.show()


fig, axes = plt.subplots(nrows=8, ncols=3, figsize=(7, 14), sharex='col', sharey='col')
tags = ['champions','promising','potentital_loyalist','newcome','former_loyalist','at_risk','hibernating','almost_lost']
types = ['Recency','Frequency','Monetary']
for r in range(0,8,1):
    for l in range(0,3,1):
        tag = tags[r]
        typee = types[l]
        sns.histplot(rfm_df3[rfm_df3['tags']==tag][typee], kde=True, ax=axes[r,l]) # bins=30, 
        axes[r,l].set_title(f'{typee[0]} of {tag}')
plt.tight_layout()
plt.show()








event_time_df['min_time2'] = pd.to_datetime(event_time_df['min_time'].str[:-4],utc=True, format='%Y-%m-%d %H:%M:%S').dt.tz_localize(None)
T_data = (analysis_date - event_time_df['min_time2']).dt.days+1
rfm_df4 = pd.DataFrame({'r':event_time_df['daysdiff'],'f':user_buy_times,'T':T_data})
rfm_df4 = rfm_df4.join(other=rfm_df,on=['user_id'],how='right',lsuffix='',rsuffix='r')[['r','f','T','Monetary']]#.iloc[:, :3]
rfm_df4 # 24813 will be the results of excluding extreme values instead of 25613 rows.


# remove the 0 times and 1 times bought from it
rfm_df4 = rfm_df4[rfm_df4['f']>1]
bgf = lifetimes.BetaGeoFitter(penalizer_coef=0.05)
bgf.fit(rfm_df4['f'],rfm_df4['r'],rfm_df4['T'])
print(bgf)
bgf.summary


lifetimes.plotting.plot_frequency_recency_matrix(bgf)


lifetimes.plotting.plot_probability_alive_matrix(bgf)


rfm_df4['probability_alive'] = bgf.conditional_probability_alive(rfm_df4['f'], rfm_df4['r'], rfm_df4['T'])
rfm_df4['predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(40, rfm_df4['f'], rfm_df4['r'], rfm_df4['T'])
rfm_df4.sort_values(by='predicted_purchases',ascending=False)


lifetimes.plotting.plot_period_transactions(bgf, max_frequency=11)


rfm_df4[['f','Monetary']].corr()


rfm_df4_standardized = (rfm_df4[['f','Monetary']] - rfm_df4[['f','Monetary']].mean()) / rfm_df4[['f','Monetary']].std()
pca = PCA(n_components='mle')
pca.fit(rfm_df4_standardized)
rfm_df4_pca = pca.transform(rfm_df4_standardized)
rfm_df4_pca = pd.DataFrame(rfm_df4_pca, columns=['PCA_Dimension_1'])
rfm_df4_combined = pd.concat([rfm_df4, rfm_df4_pca], axis=1)
rfm_df4











from lifetimes.datasets import load_cdnow_summary_data_with_monetary_value

summary_with_money_value = load_cdnow_summary_data_with_monetary_value()
summary_with_money_value.head()
returning_customers_summary = summary_with_money_value[summary_with_money_value['frequency']>0]

print(returning_customers_summary.head())
returning_customers_summary[['monetary_value', 'frequency']].corr()


from lifetimes import GammaGammaFitter

ggf = GammaGammaFitter(penalizer_coef = 0)
ggf.fit(returning_customers_summary['frequency'],
        returning_customers_summary['monetary_value'])
print(ggf)


print(ggf.conditional_expected_average_profit(
        summary_with_money_value['frequency'],
        summary_with_money_value['monetary_value']
    ).head(10))


print("Expected conditional average profit: %s, Average profit: %s" % (
    ggf.conditional_expected_average_profit(
        summary_with_money_value['frequency'],
        summary_with_money_value['monetary_value']
    ).mean(),
    summary_with_money_value[summary_with_money_value['frequency']>0]['monetary_value'].mean()
))


# refit the BG model to the summary_with_money_value dataset
bgf.fit(summary_with_money_value['frequency'], summary_with_money_value['recency'], summary_with_money_value['T'])

print(ggf.customer_lifetime_value(
    bgf, #the model to use to predict the number of future transactions
    summary_with_money_value['frequency'],
    summary_with_money_value['recency'],
    summary_with_money_value['T'],
    summary_with_money_value['monetary_value'],
    time=12, # months
    discount_rate=0.01 # monthly discount rate ~ 12.7% annually
).head(10))

















print('total_uv = '+str(total_uv))
print('total_pv = '+str(total_pv))
print('avg page view per user: ',round(total_pv/total_uv,2))
# we have variable: event_frequency = data['user_id'].value_counts() = data.groupby('user_id').count()['user_session']
event_frequency


data['event_time'][0]


date = []
hour = []

for i in range(0,total_pv):
    split_time = str(data['event_time'][i]).split(' ')
    for a in split_time:
        if "-" in a:
            date.append(a)
        elif ":" in a:
            hour.append(a)
# print(date,hour)


data['date']=date
data['hour']=hour
# del data['event_time']
print(data['date'].tail(10),data['hour'].head(10))


# daily traffic
daily_pv=data[data.event_type=='view'].groupby('date')['user_id'].count().reset_index().rename(columns={'user_id':'daily_pv'})
daily_uv=data[data.event_type=='view'].groupby('date')['user_id'].nunique().reset_index().rename(columns={'user_id':'daily_uv'})
print(daily_pv,'\n\n',daily_uv)


ax1=plt.subplot(211,label='daily_pv')
ax2=plt.subplot(212,sharex=ax1,label='daily_uv')

daily_pv.plot(x='date',y='daily_pv',ax=ax1,rot=45,fontsize=10,title='daily_pv')
daily_uv.plot(x='date',y='daily_uv',ax=ax2,rot=45,fontsize=10,title='daily_uv')





# hourly traffic (total traffic of all days in hourly granularity

# for i in data.columns[:2]:
#     data[i]=data[i].astype('str')
# data['date']=pd.to_datetime(data.date)
# data['hour']=data['hour'].astype('int')
hour_int=[]
for i in hour:
    hour_int.append(i[0:2])
data['hour_int']=hour_int
hourly_pv=data[data.event_type=='view'].groupby('hour_int')['user_id'].count().reset_index().rename(columns={'user_id':'hourly_pv'})
hourly_uv=data[data.event_type=='view'].groupby('hour_int')['user_id'].nunique().reset_index().rename(columns={'user_id':'hourly_uv'})
print(hourly_pv,'\n\n',hourly_uv)


ax1=plt.subplot(211,label='hourly_pv')
ax2=plt.subplot(212,sharex=ax1,label='hourly_uv')

hourly_pv.plot(x='hour_int',y='hourly_pv',ax=ax1,rot=45,fontsize=10,title='hourly_pv')
hourly_uv.plot(x='hour_int',y='hourly_uv',ax=ax2,rot=45,fontsize=10,title='hourly_uv')
# axes[0].xaxis.set_minor_locator(ticker.MultipleLocator(1))
# axes[1].xaxis.set_minor_locator(ticker.MultipleLocator(1))


# series of indicators
mean_daily_pv = daily_pv['daily_pv'].mean()
print('mean_daily_pv = '+str('%.2f'%mean_daily_pv))
mean_daily_uv = daily_uv['daily_uv'].mean()
print('mean_daily_uv = '+str('%.2f'%mean_daily_uv))


mean_daily_gmv = data.groupby('date')['price'].sum().mean()
print('mean_daily_gmv = '+str('%.2f'%mean_daily_gmv))
mean_daily_order = data[data.event_type=='purchase'].groupby('date')['product_id'].nunique().values.mean() #series.values访问频数，.index访问索引
print('mean_daily_order = '+str('%.0f'%mean_daily_order))
mean_daily_paid_user = data[data.event_type=='purchase'].groupby('date')['user_id'].nunique().mean()
print('mean_daily_paid_user = '+str('%.0f'%mean_daily_paid_user))

mean_daily_avg_order_per_paid_user = mean_daily_order/mean_daily_paid_user
print('mean_daily_avg_order_per_paid_user = '+str('%.0f'%mean_daily_avg_order_per_paid_user))
mean_daily_AOV = mean_daily_gmv/mean_daily_order
print('mean_daily_avg_order_value = '+str('%.2f'%mean_daily_AOV))
mean_daily_paid_rate = (mean_daily_paid_user/mean_daily_uv)*100
print('mean_daily_paid_rate = '+str('%.2f'%mean_daily_paid_rate)+'%')

mean_daily_ARPU = (mean_daily_gmv/mean_daily_uv)
print('mean_daily_ARPU = '+str('%.2f'%mean_daily_ARPU))
mean_daily_ARPPU = (mean_daily_gmv/mean_daily_paid_user)
print('mean_daily_ARPPU = '+str('%.2f'%mean_daily_ARPPU))


mean_hourly_pv = hourly_pv['hourly_pv'].mean()
print('mean_hourly_pv = '+str('%.2f'%mean_hourly_pv))
mean_hourly_uv = hourly_uv['hourly_uv'].mean()
print('mean_hourly_uv = '+str('%.2f'%mean_hourly_uv))


mean_hourly_gmv = data.groupby('hour_int')['price'].sum().mean()
print('mean_hourly_gmv = '+str('%.2f'%mean_hourly_gmv))
mean_hourly_order = data[data.event_type=='purchase'].groupby('hour_int')['product_id'].nunique().values.mean() #series.values访问频数，.index访问索引
print('mean_hourly_order = '+str('%.0f'%mean_hourly_order))
mean_hourly_paid_user = data[data.event_type=='purchase'].groupby('hour_int')['user_id'].nunique().mean()
print('mean_hourly_paid_user = '+str('%.0f'%mean_hourly_paid_user))

mean_hourly_avg_order_per_paid_user = mean_hourly_order/mean_hourly_paid_user
print('mean_hourly_avg_order_per_paid_user = '+str('%.0f'%mean_hourly_avg_order_per_paid_user))
mean_hourly_AOV = mean_hourly_gmv/mean_hourly_order
print('mean_hourly_avg_order_value = '+str('%.2f'%mean_hourly_AOV))
mean_hourly_paid_rate = (mean_hourly_paid_user/mean_hourly_uv)*100
print('mean_hourly_paid_rate = '+str('%.2f'%mean_hourly_paid_rate)+'%')

mean_hourly_ARPU = (mean_hourly_gmv/mean_hourly_uv)
print('mean_hourly_ARPU = '+str('%.2f'%mean_hourly_ARPU))
mean_hourly_ARPPU = (mean_hourly_gmv/mean_hourly_paid_user)
print('mean_hourly_ARPPU = '+str('%.2f'%mean_hourly_ARPPU))





heatmap_data = data[data.event_type=='view'].groupby(['date','hour_int']).agg({'user_id':'count'}).reset_index().rename(columns={'user_id':'unit_pv'}).pivot(index='hour_int', columns='date')['unit_pv']
heatmap_data


# pyecharts
# value = [[heatmap_data.columns, heatmap_data.index, heatmap_data.values]]
# c = (
#     HeatMap()
#     .add_xaxis(Faker.clock)
#     .add_yaxis(
#         "series0",
#         Faker.week,
#         value,
#         label_opts=opts.LabelOpts(is_show=True, position="inside"),
#     )
#     .set_global_opts(
#         title_opts=opts.TitleOpts(title="monthly traffic heatmap"),
#         visualmap_opts=opts.VisualMapOpts(),
#     )
#     .render("monthly_traffic_heatmap.html")
# )


final_list = []
for i in heatmap_data.columns.tolist():
    date_object = dt.strptime(i, '%Y-%m-%d')
    weekday = date_object.weekday()+1
    final_str = i+' wkd'+str(weekday)
    final_list.append(final_str)
# print(f"The day of the week for {date_string} is {weekday}.")


# Draw a heatmap with the numeric values in each cell
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,22)) #, sharex=True
# plt.title('Traffic Summary in Dec. 2019')
sns.heatmap(heatmap_data, cmap="RdBu_r", annot=True, fmt="d", annot_kws={'fontsize':8}, linewidths=0, ax=ax1, xticklabels=final_list)
sns.heatmap(heatmap_data, vmin=3000, annot=True, fmt="d", linewidths=0, annot_kws={'fontsize':8}, ax=ax2, xticklabels=False)
plt.show()
# annotation formatting
#.1e = scientific notation with 1 decimal point (standard form)
# .2f = 2 decimal places
# .3g = 3 significant figures
# .4% = percentage with 4 decimal places











# we have variable: event_frequency = data['user_id'].value_counts() = data.groupby('user_id').count()['user_session']
# we have variable: event_type_df = data.groupby(['user_id', 'event_type']).count().reset_index()
event_type_df
event_type_df.pivot(index='user_id',columns='event_type',values='event_time').fillna(0).astype(int)


data['user_session'][data['user_session'].duplicated().values==True]
session_type_df = data.groupby(['user_session', 'event_type']).count()['event_time'].reset_index()
session_type_df = session_type_df.pivot(index='user_session',columns='event_type',values='event_time').fillna(0).astype(int).iloc[:-1,:]
session_type_df


v2c = session_type_df[(session_type_df.view>0) & (session_type_df.cart>0)].count()['cart']
v2p = session_type_df[(session_type_df.view>0) & (session_type_df.cart==0) & (session_type_df.purchase>0)].count()['cart']
v2r = session_type_df[(session_type_df.view>0) & (session_type_df.remove_from_cart>0) & (session_type_df.purchase==0)].count()['cart']
c2p = session_type_df[(session_type_df.view==0) & (session_type_df.cart>0) & (session_type_df.purchase>0)].count()['cart']
r2p = session_type_df[(session_type_df.view==0) & (session_type_df.cart==0) & (session_type_df.remove_from_cart>0)  & (session_type_df.purchase>0)].count()['cart']
onlyv = session_type_df[(session_type_df.view>0) & (session_type_df.cart==0) & (session_type_df.remove_from_cart==0) & (session_type_df.purchase==0)].count()['cart']
onlyc = session_type_df[(session_type_df.view==0) & (session_type_df.cart>0) & (session_type_df.remove_from_cart==0) & (session_type_df.purchase==0)].count()['cart']
onlyp = session_type_df[(session_type_df.view==0) & (session_type_df.cart==0) & (session_type_df.remove_from_cart==0) & (session_type_df.purchase>0)].count()['cart']
onlyr = session_type_df[(session_type_df.view==0) & (session_type_df.cart==0) & (session_type_df.remove_from_cart>0) & (session_type_df.purchase==0)].count()['cart']

print(v2c,v2p,v2r,c2p,r2p,onlyv,onlyc,onlyp,onlyr)
# data[data['user_session'] == '00002b0e-d7f7-454e-8386-431c4021a9f6'].sort_values(by='event_time')


sankey_data = {
    'source': ['view', 'view', 'view', 'cart', 'remove_from_cart', 'view', 'cart', 'purchase','remove_from_cart'],
    'target': ['cart', 'purchase', 'remove_from_cart', 'purchase', 'purchase', 'view','cart','purchase','remove_from_cart'],
    'value': [v2c,v2p,v2r,c2p,r2p,onlyv,onlyc,onlyp,onlyr]
}
sankey_df = pd.DataFrame(sankey_data)
sankey_df['pct'] = round((sankey_df['value']/len(session_type_df)),4).apply(lambda x: '{:.2%}'.format(x))
sankey_df['pct'].values


fig = go.Figure(data=[go.Sankey(
                            node = dict(
                                pad = 15,
                                thickness = 20,
                                line = dict(color = "black", width = 0.5),
                                label = ["view", "cart", "rm_from_cart", "purchase"],
                                hovertemplate='Node %{label} has total value %{value}<extra></extra>',
                                color = "green"
                            ),
                            link = dict(
                                source = [0,0,0,1,2,0,1,2,3],
                                target = [1,2,3,3,3,0,1,2,3],
                                value = [v2c,v2r,v2p,c2p,r2p,onlyv,onlyc,onlyr,onlyp],
                                customdata = [0.1549, 0.0022, 0.0622, 0.0045, 0.0007, 0.778 , 0.0277, 0.0034,0.0081],
                                hovertemplate='Link from %{source.label}<br />'+'to %{target.label}<br />has value %{value}'+'<br /> and pct %{customdata}<extra></extra>',
                            )
                        )
                     ]
               )

fig.update_layout(title_text="User Behavior Path Flow SankeyDiagram", font_size=20)
fig.show()


# data.groupby('user_session','user_id','product_id').agg().sort_values('user_session','event_time')


# from pyecharts import options as opts
# from pyecharts.charts import Sankey

# sankey_data2 = []
# for index, row in sankey_df.iterrows():
#     sankey_data2.append({"source": row['source'], "target": row['target'], "value": row['value']})

# sankey = (
#     Sankey()
#     .add(
#         "User Behavior Path Flow",
#         nodes=['view', 'cart', 'remove_from_cart', 'purchase'],
#         links=sankey_data2,
#         itemstyle_opts=opts.ItemStyleOpts(border_width=1, border_color="#aaa"),
#         linestyle_opt=opts.LineStyleOpts(color="source", curve=0.5, opacity=0.5),
#         tooltip_opts=opts.TooltipOpts(trigger_on="mousemove")
#     )
#     .set_global_opts(title_opts=opts.TitleOpts(title="User Behavior Path Flow SankeyDiagram"))
#     # .render_notebook("ecommerce_user_behavior_sankey.html")
# )

# sankey.load_javascript()
# sankey.render_notebook()





























brand_df = data[data.brand!='null'].groupby('brand').agg({'product_id': 'count', 'price': ['mean','sum'], 'user_id': pd.Series.nunique}) # brand_df['order_value'] = data[data.event_type=='purchase']
brand_df.columns = ['event_times','event_value','gross_event_volume','user_cnt'] # brand_df.columns.droplevel(0)
# , 'category_id': lambda x: ','.join(x.unique())

fig = px.scatter(brand_df, x='user_cnt', y='event_times', size="gross_event_volume", 
                 labels={'heat':'event_times','user_cnt':'user_cnt'}, 
                 hover_name=brand_df.index, size_max=30, title='Heat of Brands')
# hover_data={'price': True}, color="continent",  log_x=True,
fig.show()
# brand_df


top_event_value = brand_df.sort_values(by='event_value',ascending=False).head(20)
top_event_value
top_gev = brand_df.sort_values(by='gross_event_volume',ascending=False).head(20)
top_gev

brand_event_df = data[data.brand!='null'].groupby(['brand', 'event_type']).agg({'product_id':'count', 'user_id': 'nunique','price':['mean','sum']})
brand_event_df.columns = ['spec_times','user_cnt','AOV','gmv']
# .rename(columns={'product_id':'spec_times','user_id':})
# pd.Series.nunique
brand_event_df['spec_times_peruser'] = brand_event_df['spec_times'] / brand_event_df['user_cnt']
# brand_event_df['AOV'] = brand_event_df['gmv'] / brand_event_df['spec_times']
brand_event_df['gmv_peruser'] = brand_event_df['gmv'] / brand_event_df['user_cnt']
brand_event_df


brand_event_df.reset_index(inplace=True)

top_view = brand_event_df[brand_event_df['event_type']=='view'].sort_values(by='spec_times', ascending=False)
top_cart = brand_event_df[brand_event_df['event_type']=='cart'].sort_values(by='spec_times', ascending=False)
top_buy = brand_event_df[brand_event_df['event_type']=='purchase'].sort_values(by='spec_times', ascending=False)
top_rm_from_cart = brand_event_df[brand_event_df['event_type']=='remove_from_cart'].sort_values(by='spec_times', ascending=False)
# top_order_value = brand_event_df[brand_event_df['event_type']=='purchase'].sort_values(by='event_value', ascending=False)
top_gmv = brand_event_df[brand_event_df['event_type']=='purchase'].sort_values(by='gmv', ascending=False)
top_AOV = brand_event_df[brand_event_df['event_type']=='purchase'].sort_values(by='AOV', ascending=False)

# print('top_view:\n',top_view.head(20),
#       '\n\ntop_cart:\n',top_cart.head(20),
#       '\n\ntop_buy:\n',top_buy.head(20),
#       '\n\ntop_rm_from_cart:\n',top_rm_from_cart.head(20),
#       '\n\ntop_AOV:\n',top_AOV.head(20),
#       '\n\ntop_gmv:\n',top_gmv.head(20)
#      )


fig = px.scatter(top_view, x='user_cnt', y='spec_times', size="gmv", 
                 labels={'spec_times':'event_times','user_cnt':'user_cnt'}, 
                 hover_name=top_view.brand, size_max=30, title='View Heat of Brands')
fig.show()
fig = px.scatter(top_cart, x='user_cnt', y='spec_times', size="gmv", 
                 labels={'spec_times':'event_times','user_cnt':'user_cnt'}, 
                 hover_name=top_cart.brand, size_max=30, title='cart Heat of Brands')
fig.show()
fig = px.scatter(top_buy, x='user_cnt', y='spec_times', size="gmv", 
                 labels={'spec_times':'event_times','user_cnt':'user_cnt'}, 
                 hover_name=top_buy.brand, size_max=30, title='Purchase Heat of Brands')
fig.show()





# top_loyalty_brand = pd.merge(top_view.nlargest(20, 'event_value'),top_view.nlargest(20, 'spec_times_peruser'),how='inner')
top_loyalty_brand = pd.concat([top_view.nlargest(20, 'spec_times_peruser'),top_cart.nlargest(20, 'spec_times_peruser'),top_buy.nlargest(20, 'spec_times_peruser')], axis=0)
top_loyalty_brand

# data['product_id'].nunique() - data[data['brand']=='null']['product_id'].nunique()
# data[(data.brand=='airnails') & (data.product_id=='5659761')]
# I found that some product_id have multiple price so i make it to mode() appeared in the first time

def first_mode(x):
    return x.mode().iloc[0]

pd_df = data[data.brand!='null'].groupby(['brand','product_id']).agg({'price': first_mode}) # pd.Series.mode
product_diversity = pd_df.groupby(level=0).count().sort_values(by='price',ascending=False).head(20).rename(columns={'price':'product_cnt'})
product_diversity

top_div_brand = product_diversity.sort_values(by='product_cnt',ascending=False).head(20)
top_div_brand

avg_unit_price =  pd_df.groupby(level=0).mean().sort_values(by='price',ascending=False)
top_unit_price = avg_unit_price.head(20)
# top_unit_price

# avg_unit_price['price_numeric'] = pd.to_numeric(avg_unit_price['price'], errors='coerce')
# non_float_rows = avg_unit_price[avg_unit_price['price_numeric'].isnull()]
# non_float_rows





lost_most_brand = top_rm_from_cart.nlargest(20,'spec_times_peruser')
lost_most_brand

brand_cvr_df = brand_event_df.pivot(index='brand', columns='event_type')['user_cnt']
brand_cvr_df


brand_cvr_df = brand_event_df.pivot(index='brand', columns='event_type')['user_cnt']# .fillna(0).astype(int)
brand_cvr_df['view2cart'] = brand_cvr_df['cart']/brand_cvr_df['view']
brand_cvr_df['cart2buy'] = brand_cvr_df['purchase']/brand_cvr_df['cart']
brand_cvr_df['view2buy'] = brand_cvr_df['purchase']/brand_cvr_df['view']
low_view2cart = brand_cvr_df.nsmallest(20,'view2cart')
low_cart2buy = brand_cvr_df.nsmallest(20,'cart2buy')
low_view2buy = brand_cvr_df.nsmallest(20,'view2buy')
# print('low_view2cart:\n',low_view2cart,
#       '\n\nlow_cart2buy:\n',low_cart2buy,
#       '\n\nlow_view2buy:\n',low_view2buy
#      )





top_gmv_peruser = brand_event_df[brand_event_df['event_type']=='purchase'].nlargest(20,'gmv_peruser')
top_gmv_peruser

def keep_top20(group):
    return group.nlargest(20)
ka_list = data[data.event_type=='purchase'].groupby(['brand','user_id'])['price'].sum().groupby('brand',group_keys=False).apply(keep_top20)
# ka_list





product_df = data.groupby(['product_id','event_type']).agg({'product_id': 'count', 'price': ['mean','sum'], 'user_id': 'nunique'})
product_df.columns = ['spec_times','AOV_times','gmv','user_cnt']
product_df['spec_times_peruser'] = product_df['spec_times']/product_df['user_cnt']
product_df['gmv_peruser'] = product_df['gmv']/product_df['user_cnt']
product_df


top_AOV_times = product_df.reset_index()[product_df.reset_index().event_type=='purchase']['AOV_times'].nlargest(20)
top_gmv_peruser = product_df.reset_index()[product_df.reset_index().event_type=='purchase']['gmv_peruser'].nlargest(20)
top_gmv = product_df.reset_index()[product_df.reset_index().event_type=='purchase']['gmv'].nlargest(20)
# print('top_AOV_times(how much order value created by per bought action in average of this product):\n',top_AOV_times,
#       '\n\ntop_gmv_peruser(how much order value created by per paid_user in average of this product):\n',top_gmv_peruser,
#      '\n\ntop_gmv:\n',top_gmv)

pd_heat = product_df.groupby(level=0)['spec_times'].sum()
top_heat_pd = pd_heat.nlargest(20)
top_heat_pd

# product_df.reset_index(inplace=True)
spec_times_df = product_df.reset_index().pivot(index='product_id', columns='event_type')['spec_times']
top_view_pd = spec_times_df['view'].nlargest(20)
top_cart_pd = spec_times_df['cart'].nlargest(20)
top_buy_pd = spec_times_df['purchase'].nlargest(20)
top_rm_pd = spec_times_df['remove_from_cart'].nlargest(20)
# print('top_view_pd:\n',top_view_pd,'\n\ntop_cart_pd:\n',top_cart_pd,'\n\ntop_buy_pd:\n',top_buy_pd,'\n\ntop_rm_pd:\n',top_rm_pd)

spec_times_peruser_df = product_df.reset_index().pivot(index='product_id', columns='event_type')['spec_times_peruser']
top_loyalty_pd = product_df['spec_times_peruser'].swaplevel().groupby('event_type',group_keys=False).apply(keep_top20)
top_loyalty_pd

top_dear_pd = pd_df.reset_index(level=0).drop('brand',axis=1).nlargest(20,'price')
# top_dear_pd








data.head()


# The matix from 44624 products *213176 users(which is a df with 25613 rows × 26815 columns) is far more out of memory limit. So we abandon Apriori and change to FP-growth algorithm in stead to avoid computational complexity and error.
basket_data = data[data.event_type=='purchase'].groupby(['user_id','product_id']).agg({'event_time':'nunique'}).unstack('product_id').fillna(0).astype(int)
basket_data.columns = basket_data.columns.droplevel(0)
basket_data.head()


basket_data


basket_data.shape


basket_sets = basket_data.apply(lambda x: np.where(x > 0, 1, 0)) #applymap(lambda x: 1 if x > 0 else 0)
basket_sets.head()


from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules


frequent_prosets = apriori(basket_sets, min_support=0.005, use_colnames=True, verbose=1, low_memory=True)
#Build frequent itemsets
frequent_prosets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
frequent_prosets


rules = association_rules(frequent_prosets, metric="lift", min_threshold=1)
rules


#Products having 60% confidence likely to be purchased together
rules[(rules['lift'] >= 6) & (rules['confidence'] >= 0.6)]





# cooc_matix = basket_sets.T.dot(basket_sets)
cooc_matix























from itertools import combinations, groupby
from collections import Counter


# Function that returns the size of an object in MB
def size(obj):
    return "{0:.2f} MB".format(sys.getsizeof(obj) / (1000 * 1000))


print('data -- dimensions: {0};   size: {1}'.format(data.shape, size(data)))
display(data.head())


basket_data = data[data.event_type=='purchase'].set_index('user_id')['product_id']#.groupby('user_id')['product_id']


print('basket_data -- dimensions: {0};   size: {1};   unique_users: {2};   unique_products: {3}'
      .format(basket_data.shape, size(basket_data), len(basket_data.index.unique()), len(basket_data.value_counts())))





def output_exl(dfs,sheet_name,k):
    i = 0
    max_col=1
    for df in dfs:
        if isinstance(df, pd.DataFrame):
            df.to_excel(excel_writer=writer, sheet_name=sheet_name,startrow=i,index=True,freeze_panes=(1,1),float_format="%.2f") # write each df into specific sheet in excel
            row = len(df.index)
            i += row + 4 # set 3 blank rows between each df
            max_col = max(len(df.columns),max_col)+1
        elif isinstance(df, pd.Series):
            df.to_frame().to_excel(excel_writer=writer, sheet_name=sheet_name,startrow=i,index=True,freeze_panes=(1,1),float_format="%.2f")
            row = len(df.index)
            i += row + 4 # set 3 blank rows between each df
            max_col = max(len(df.to_frame().columns),max_col)+1
        else:
            print("Unsupported data type")
    
    # set the col width    
    ws = writer.sheets[sheet_name]
    for j in range(max_col): #len()dfs[k].columns
        if j == 0:
            ws.column_dimensions[ascii_uppercase[j]].width = 30
        else:
            ws.column_dimensions[ascii_uppercase[j]].width = 20


writer = pd.ExcelWriter('./formated_data_for_marketing_{date}.xlsx'.format(date=dt.today().strftime('%Y%m%d')), engine='openpyxl')#,if_sheet_exists='new',mode='a'

output_exl(dfs = [
    top_gev,
    top_view.head(20),
    top_cart.head(20),
    top_buy.head(20),
    top_rm_from_cart.head(20),
    top_gmv, #
    top_AOV.head(20), 
    top_div_brand,
    top_unit_price,
    lost_most_brand,
    low_view2cart,
    low_cart2buy,
    low_view2buy,
    top_gmv_peruser #
    ],sheet_name = 'brand',k=1)

output_exl(dfs = [
    top_AOV_times,
    top_gmv_peruser,
    top_gmv,
    top_heat_pd,
    top_view_pd,
    top_cart_pd,
    top_buy_pd,
    top_rm_pd,
    top_dear_pd
    
    ],sheet_name = 'product',k=-1)

output_exl(dfs = [spec_times_df],sheet_name = 'spec_times_of_products',k=0)
output_exl(dfs = [brand_event_df],sheet_name = 'overview_of_brands',k=0)
output_exl(dfs = [ka_list],sheet_name = 'KA_list_of_brands',k=0)
output_exl(dfs = [brand_cvr_df],sheet_name = 'conversion_rate_of_brands',k=0)
output_exl(dfs = [top_loyalty_brand],sheet_name = 'top_loyalty_brands',k=0)
output_exl(dfs = [top_loyalty_pd],sheet_name = 'top_loyalty_products',k=0)

writer.close()
print('check file: ./formated_data_for_marketing_{date}.xlsx'.format(date=dt.today().strftime('%Y%m%d')))





def send_email_report_with_attachment(file_list, subject=None, body=__name__, to_list=[],
                                      cc_list=[],
                                      mail_type='bacc_report'):
    if not subject:
        subject = '监控报告'
    if not to_list:
        to_list = ['litingting@wecash.net']

    smtpserver = const.MAIL_SERVER
    mail_user = const.MAIL_ACCOUNT[mail_type]['user']
    mail_passwd = const.MAIL_ACCOUNT[mail_type]['pwd']
    msg = MIMEMultipart()
    body = MIMEText(body, 'html', 'utf-8')
    msg.attach(body)
    for f in file_list:
        with open(f, 'rb') as file:
            att = MIMEText(file.read(), 'base64', 'utf-8')
            att['Content-Type'] = 'application/octet-stream'
            att.add_header('Content-Disposition', 'attachment', filename=('gbk', '', f.split('/')[-1]))
            # att['Content-Disposition'] = 'attachment; filename={fileName}'.format(
            #     fileName=f.split('/')[-1])
            msg.attach(att)
            file.close
    msg['From'] = mail_user
    msg['To'] = ",".join(to_list)
    msg['Cc'] = ",".join(cc_list)
    msg['Subject'] = Header(subject, 'utf-8')
    smtp = smtplib.SMTP_SSL()
    smtp.connect(smtpserver, 465)
    smtp.login(mail_user, mail_passwd)
    smtp.sendmail(mail_user, to_list + cc_list, msg.as_string())
    print('send')


def send_email_with_attachment():
    # overview
    df = data_in1()
    ## excel 
    df_overview = df.groupby('period').sum().reset_index()
    df_overview['avg_tx_customer_tx_counts'] = df_overview['tx_case_counts'] / df_overview['tx_customer_counts']
    df_overview['avg_tx_counts_tx_amounts'] = df_overview['total_amounts'] / df_overview['tx_case_counts']
    df_overview['avg_tx_customer_fees'] = df_overview['total_fees'] / df_overview['tx_customer_counts']
    ## body
    df_overview_show = df_overview.iloc[:,:-4]
    df_overview_show['content'] = 'Overview'
    df_overview_show.iloc[:,1:-1] = df_overview_show.iloc[:,1:-1].applymap(lambda x: round(x,2))
    df_overview_show = df_overview_show.pivot_table(index='content',columns='period',
                                                    values=df_overview_show.columns[1:-1],
                                                    fill_value=0).stack(0)
    for col in range(len(df_overview_show.index)):
        df_overview_show.iloc[col] = df_overview_show.iloc[col].apply(lambda x: format(x,','))
    df_overview_show = df_overview_show.sort_index(axis=1,ascending=False).replace([0, '0.00%', '0.0'], '')


# 绘图 
    df_list = []
    df_attach = []
    body = '<p>Hi All,<p>' + 'nova_daily_report_email:<p>'
    df_summary = summary.set_index(['content', 'type'])
    ht = common_plot.HtmlTableMulti(column_width=[150,230] + len(df_summary.columns)*[110])
    t = ht.get_html_table_multi(df_summary)
    body += t
    df_list.append(df_summary)
    
    body += '<p>The daily-Transaction-Volum is:<p>'
    df_trans = df_trans.set_index('period')
    ht = common_plot.HtmlTableMulti(index_bg_color=[['white']],data_align=['center'],
                                    column_width=len(df_trans.columns)*[80])
    t = ht.get_html_table_multi(df_trans)
    body += t
    df_list.append(df_trans)
    
    
    
    file_name = os.path.join(const.PATH_XLS, 
                             'nova_daily_report_email_{date}.xlsx'.format(date=today.strftime('%Y-%m-%d')))
    writer = pd.ExcelWriter(file_name, engine='openpyxl')
    
    dfs = [summary,df_overview,df_funnel,df_retention,df_transaction,df_trade,df_conv]
    
    sheetname_list=['summary','overview', 'Conversion Funnel', 'Customer Retention',
                    'Transactions by Type','Channel_Trade_Contribution','Channel_Conversion_Contribution']
    i = 0
    for df in dfs:
        df.to_excel(excel_writer=writer, sheet_name=sheetname_list[i],freeze_panes=(1,1),index=False)
        for j in range(len(df.columns)):
            writer.sheets[sheetname_list[i]].column_dimensions[ascii_uppercase[j]].width = 30
        i = i + 1
    
    writer.save()

    to_list = ['lais@novadax.com','novadax_operation@wecash.net','novadax_product@wecash.net',
               'zhoumaomao@wecash.net', 'antonio@novadax.com',
               'zhengerkui@wecash.net', 'sunxiaomei@wecash.net','cesar@novadax.com',
               'larissa@novadax.com','matheus@novadax.com','aloizio@novadax.com','thais@novadax.com']
    # to_list = ['chentingting@wecash.net']
    cc_list = ['chentingting@wecash.net']
    
    
    common_func.send_email_report_with_attachment(file_list=[file_name], 
                                                  subject='nova_daily_report_email_{date}'.format(date=today.strftime('%Y-%m-%d')),
                                                  body=body,
                                                  to_list=to_list, 
                                                  cc_list=cc_list,
                                                  mail_type='bacc_monitor')
    
if __name__ == '__main__':
    current_module = sys.modules[__name__]
    data_util.update_data(source_module=current_module)
    send_email_with_attachment()






